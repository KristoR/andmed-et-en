"""Promote high-confidence candidate terms into the glossary.

Reads data/candidate_terms.yml (generated by fetch_theses.py) and adds
qualifying entries to data/terms.yml. Designed to be run both manually
and from the GitHub Actions harvest workflow.

Usage:
    uv run python scripts/promote_candidates.py [OPTIONS]

Options:
    --min-frequency N   Only promote terms mentioned in N+ theses (default: 1)
    --candidates PATH   Path to candidate_terms.yml (default: data/candidate_terms.yml)
    --dry-run           Show what would be added without modifying terms.yml
"""

from __future__ import annotations

import argparse
import sys
from pathlib import Path

import yaml

ROOT_DIR = Path(__file__).resolve().parents[1]
DATA_DIR = ROOT_DIR / "data"
TERMS_FILE = DATA_DIR / "terms.yml"
CANDIDATES_FILE = DATA_DIR / "candidate_terms.yml"


def load_existing_en_terms(path: Path) -> set[str]:
    """Return set of existing EN terms (lowercased) to avoid duplicates."""
    if not path.exists():
        return set()
    raw = yaml.safe_load(path.read_text(encoding="utf-8"))
    if not raw or not isinstance(raw, list):
        return set()
    terms: set[str] = set()
    for item in raw:
        if isinstance(item, dict):
            en = str(item.get("en", "")).strip().lower()
            if en:
                terms.add(en)
    return terms


def load_candidates(path: Path) -> list[dict]:
    """Load candidate terms from YAML file."""
    if not path.exists():
        return []
    raw = yaml.safe_load(path.read_text(encoding="utf-8"))
    if not raw or not isinstance(raw, list):
        return []
    return raw


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Promote high-confidence candidate terms into the glossary.",
    )
    parser.add_argument(
        "--min-frequency",
        type=int,
        default=1,
        help="Only promote terms mentioned in N+ theses (default: 1)",
    )
    parser.add_argument(
        "--candidates",
        type=Path,
        default=CANDIDATES_FILE,
        help=f"Path to candidate_terms.yml (default: {CANDIDATES_FILE.relative_to(ROOT_DIR)})",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be added without modifying terms.yml",
    )
    args = parser.parse_args()

    # Load existing terms
    existing = load_existing_en_terms(TERMS_FILE)
    print(f"Existing glossary terms: {len(existing)}")

    # Load candidates
    candidates = load_candidates(args.candidates)
    if not candidates:
        print("No candidate terms found. Run fetch_theses.py first.")
        return

    print(f"Candidate terms loaded: {len(candidates)}")

    # Filter candidates
    to_promote: list[dict] = []
    for candidate in candidates:
        en = str(candidate.get("en", "")).strip()
        if not en:
            continue

        # Only promote high-confidence (curated list matches)
        confidence = str(candidate.get("confidence", "")).strip()
        if confidence != "high":
            continue

        # Check frequency threshold
        frequency = int(candidate.get("frequency", 0))
        if frequency < args.min_frequency:
            continue

        # Skip if already in glossary
        if en.lower() in existing:
            continue

        # Need at least one Estonian translation
        et_hints = candidate.get("et_hints", [])
        if not et_hints:
            continue

        to_promote.append(candidate)

    if not to_promote:
        print("No new terms to promote.")
        return

    print(f"\nTerms to promote: {len(to_promote)}")
    for candidate in to_promote:
        en = candidate["en"]
        et_hints = candidate.get("et_hints", [])
        freq = candidate.get("frequency", 0)
        print(f"  + {en} → {et_hints[0]} ({freq} mentions)")

    if args.dry_run:
        print("\n(dry run — no changes made)")
        return

    # Load current terms.yml
    if TERMS_FILE.exists():
        current_entries = yaml.safe_load(TERMS_FILE.read_text(encoding="utf-8")) or []
    else:
        current_entries = []

    # Add promoted terms
    for candidate in to_promote:
        en = candidate["en"]
        et_hints = candidate.get("et_hints", [])

        et_primary = et_hints[0]
        et_alternatives = et_hints[1:] if len(et_hints) > 1 else []

        entry: dict = {
            "en": en,
            "et": et_primary,
            "alt": {
                "et": et_alternatives,
                "en": [],
            },
        }

        # Carry over thesis references if available
        sample_theses = candidate.get("sample_theses", [])
        if sample_theses:
            theses = []
            for thesis in sample_theses:
                t: dict = {}
                if thesis.get("author"):
                    t["author"] = thesis["author"]
                if thesis.get("title_et"):
                    t["title_et"] = thesis["title_et"]
                if thesis.get("title_en"):
                    t["title_en"] = thesis["title_en"]
                if thesis.get("year"):
                    t["year"] = thesis["year"]
                if thesis.get("url"):
                    t["url"] = thesis["url"]
                if t:
                    theses.append(t)
            if theses:
                entry["theses"] = theses

        current_entries.append(entry)
        existing.add(en.lower())

    # Write back
    with TERMS_FILE.open("w", encoding="utf-8") as f:
        yaml.dump(
            current_entries,
            f,
            allow_unicode=True,
            default_flow_style=False,
            sort_keys=False,
        )

    print(f"\nPromoted {len(to_promote)} terms. Total glossary terms: {len(current_entries)}")


if __name__ == "__main__":
    main()
